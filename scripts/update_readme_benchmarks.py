#!/usr/bin/env python3
"""
Automatically update README.md with the latest benchmark results.
Reads the comparison markdown generated by the analyzer and injects it.
"""

import logging
import re
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ReadmeUpdater:
    """Updates README.md with benchmark tables."""

    def __init__(self, readme_path: str = "README.md", results_path: str = "benchmark_results/lm_eval/COMPARISON.md"):
        self.readme_path = Path(readme_path)
        self.results_path = Path(results_path)

    def update(self):
        """Perform the update operation."""
        if not self.results_path.exists():
            logger.error(f"Results file not found: {self.results_path}")
            return False

        if not self.readme_path.exists():
            logger.error(f"README file not found: {self.readme_path}")
            return False

        # Read content
        results_content = self.results_path.read_text()
        readme_content = self.readme_path.read_text()

        # Extract table from results
        # Look for the table after "Performance Comparison"
        table_match = re.search(r"### Performance Comparison\n\n(.*?)\n\n###", results_content, re.DOTALL)
        if not table_match:
            # Fallback: try taking everything after the header if no next header
            table_match = re.search(r"### Performance Comparison\n\n(.*)", results_content, re.DOTALL)

        if not table_match:
            logger.error("Could not find results table in comparison file")
            return False

        table_content = table_match.group(1).strip()

        # Prepare new section content
        new_section = "<!-- LM_EVAL_RESULTS_START -->\n"
        new_section += "## LM-Eval Benchmark Results\n\n"
        new_section += "**Evaluation Harness**: lm-eval v0.4+  \n"
        new_section += "**Hardware**: M1 Mac Mini  \n"
        new_section += "**Run Mode**: Local Ollama Inference\n\n"
        new_section += "### Performance Comparison\n\n"
        new_section += table_content + "\n\n"
        new_section += f"[Full Results â†’]({self.results_path})\n"
        new_section += "<!-- LM_EVAL_RESULTS_END -->"

        # Regex to find existing block
        pattern = r"<!-- LM_EVAL_RESULTS_START -->.*?<!-- LM_EVAL_RESULTS_END -->"

        if re.search(pattern, readme_content, re.DOTALL):
            # Replace existing block
            new_readme = re.sub(pattern, new_section, readme_content, flags=re.DOTALL)
            logger.info("Updated existing benchmark section")
        else:
            # Append if not found (or insert before relevant section)
            # Try to insert before Contributing or License
            insert_marker = "## Contributing"
            if insert_marker in readme_content:
                new_readme = readme_content.replace(insert_marker, f"{new_section}\n\n{insert_marker}")
                logger.info("Inserted benchmark section before Contributing")
            else:
                new_readme = readme_content + "\n\n" + new_section
                logger.info("Appended benchmark section to end")

        # Write back
        self.readme_path.write_text(new_readme)
        logger.info(f"Successfully updated {self.readme_path}")
        return True

if __name__ == "__main__":
    updater = ReadmeUpdater()
    updater.update()

